{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y5kSbj89PU3P"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/c0/l59pm8pn7b3_pzpm0n3qhqcc0000gn/T/ipykernel_94733/4255770686.py:6: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# Define the neural network architecture\n",
        "class ParallelNN(nn.Module):\n",
        "    def __init__(self, input1_size, input2_size, hidden_size, num_classes):\n",
        "        super(ParallelNN, self).__init__()\n",
        "\n",
        "        # Define the first pipeline\n",
        "        self.pipeline1 = nn.Sequential(\n",
        "            nn.Linear(input1_size, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(2048, hidden_size)\n",
        "        )\n",
        "\n",
        "        # Define the second pipeline\n",
        "        self.pipeline2 = nn.Sequential(\n",
        "            nn.Linear(input2_size, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(2048, hidden_size)\n",
        "        )\n",
        "\n",
        "        # Define the final classification layer\n",
        "        self.classification = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_size, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.pipeline1(x1)\n",
        "        out2 = self.pipeline2(x2)\n",
        "\n",
        "        # Concatenate the outputs\n",
        "        merged_out = torch.cat((out1, out2), dim=1)\n",
        "\n",
        "        # Apply the final classification layer\n",
        "        final_out = self.classification(merged_out)\n",
        "        return final_out\n",
        "    \n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "\n",
        "    def should_stop(self, current_loss):\n",
        "        if self.best_loss is None or current_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "        \n",
        "        return self.counter >= self.patience\n",
        "\n",
        "early_stopping = EarlyStopping(patience=5, min_delta=0.001)\n",
        "\n",
        "\n",
        "# Define the input sizes, hidden size, and number of classes\n",
        "#input1_size = 10  # Change this to match the number of features in your first dataset\n",
        "#input2_size = 8   # Change this to match the number of features in your second dataset\n",
        "#hidden_size = 64\n",
        "#num_classes = 3   # Change this to match the number of classes in your classification task\n",
        "\n",
        "# Initialize the model\n",
        "#model = ParallelNN(input1_size, input2_size, hidden_size, num_classes)\n",
        "\n",
        "# Define your loss function and optimizer\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pHelC-dyPM4b"
      },
      "outputs": [],
      "source": [
        "# Assuming you have data1 and data2 as torch Tensors\n",
        "# Replace this with your actual data\n",
        "# data1 = torch.randn(10000, 10)  # Example: 100 samples, 10 features\n",
        "# data2 = torch.randn(10000, 8)   # Example: 100 samples, 8 features\n",
        "# labels = torch.randint(0, 3, (10000,))  # Example labels (0, 1, or 2)\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "# Load the data from a CSV file\n",
        "csv_name = '../data/preprocessed_data.csv'\n",
        "data = pd.read_csv(csv_name)\n",
        "\n",
        "# Normalize the data within each column except the first\n",
        "data.iloc[:, 1:] = (data.iloc[:, 1:] - data.iloc[:, 1:].mean()) / data.iloc[:, 1:].std()\n",
        "\n",
        "\n",
        "# The first column is irrelevant, the second column is the label\n",
        "# Divide the remaining columns into two datasets\n",
        "NMR_FIRST_FEATURE = 60\n",
        "data1 = torch.tensor(data.iloc[:, 1:NMR_FIRST_FEATURE - 1].values, dtype=torch.float32)\n",
        "data2 = torch.tensor(data.iloc[:, NMR_FIRST_FEATURE - 1:].values, dtype=torch.float32)\n",
        "labels = torch.tensor(data.iloc[:, 0].values, dtype=torch.long)\n",
        "\n",
        "# Define model, loss function, and optimizer\n",
        "model = ParallelNN(input1_size=data1.shape[1], input2_size=data2.shape[1], hidden_size=128, num_classes=3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "k_folds = 5\n",
        "\n",
        "# Prepare data loaders\n",
        "dataset = TensorDataset(data1, data2, labels)\n",
        "train_set, val_set = torch.utils.data.random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))])\n",
        "sampler = torch.utils.data.RandomSampler(train_set, replacement=True, num_samples=1000)\n",
        "train_loader = DataLoader(train_set, batch_size=32, sampler=sampler)\n",
        "val_loader = DataLoader(val_set, batch_size=16, shuffle=True)\n",
        "\n",
        "# Training loop\n",
        "def train(model, dataloader, criterion, optimizer):\n",
        "    pbar = tqdm(dataloader)\n",
        "    correct = 0\n",
        "\n",
        "    for (data1, data2, labels) in pbar:\n",
        "        model.train()\n",
        "\n",
        "        data1, data2, labels = data1.to(device), data2.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data1, data2)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        correct += (outputs.argmax(dim=1) == labels).float().sum()\n",
        "        accuracy = correct / 1000 # Used in sampling\n",
        "\n",
        "        \n",
        "        # Validation loop\n",
        "        val_loss = 0\n",
        "        val_accuracy = 0\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for (data1, data2, labels) in val_loader:\n",
        "                data1, data2, labels = data1.to(device), data2.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(data1, data2)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                val_accuracy += (outputs.argmax(dim=1) == labels).float().sum()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_accuracy /= len(val_set)\n",
        "         \n",
        "        pbar.set_description(f\"Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    return val_loss, val_accuracy\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 0.9494, Accuracy: 0.5970, Val Loss: 1.0434, Val Accuracy: 0.4691: 100%|██████████| 32/32 [00:00<00:00, 40.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 0.9804, Accuracy: 0.7400, Val Loss: 0.9926, Val Accuracy: 0.5679: 100%|██████████| 32/32 [00:00<00:00, 41.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 0.6714, Accuracy: 0.8520, Val Loss: 1.0096, Val Accuracy: 0.4691: 100%|██████████| 32/32 [00:00<00:00, 53.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.5901, Accuracy: 0.8820, Val Loss: 1.2019, Val Accuracy: 0.4815: 100%|██████████| 32/32 [00:00<00:00, 52.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 1.2447, Accuracy: 0.9110, Val Loss: 1.1533, Val Accuracy: 0.4938: 100%|██████████| 32/32 [00:00<00:00, 58.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 0.6941, Accuracy: 0.9290, Val Loss: 1.1947, Val Accuracy: 0.4568: 100%|██████████| 32/32 [00:00<00:00, 56.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss: 0.3470, Accuracy: 0.9230, Val Loss: 1.1249, Val Accuracy: 0.4938: 100%|██████████| 32/32 [00:00<00:00, 47.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Early stopping\n",
            "Finished training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Main function\n",
        "def main():\n",
        "    num_epochs = 30\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        val_loss, val_accuracy = train(model, train_loader, criterion, optimizer)\n",
        "        scheduler.step()\n",
        "\n",
        "        if early_stopping.should_stop(val_loss):\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    print(\"Finished training\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
